{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule as pruning_sched\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model_conv2d as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model_conv2d import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/virtualenv/matterport-tf115/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/virtualenv/matterport-tf115/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/virtualenv/matterport-tf115/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/virtualenv/matterport-tf115/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7928c4edfc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create model in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = modellib.MaskRCNN(mode=\"training\", config=config,\n\u001b[0;32m----> 3\u001b[0;31m                           model_dir=MODEL_DIR)\n\u001b[0m",
      "\u001b[0;32m~/git/Thesis_Repos/Mask_RCNN/mrcnn/model_conv2d.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[0;31m#def summary(self):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Thesis_Repos/Mask_RCNN/mrcnn/model_conv2d.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   1904\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m             _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,\n\u001b[0;32m-> 1906\u001b[0;31m                                              stage5=True, train_bn=config.TRAIN_BN)\n\u001b[0m\u001b[1;32m   1907\u001b[0m         \u001b[0;31m# Top-down Layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;31m# TODO: add assert to varify feature map sizes match what's in config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Thesis_Repos/Mask_RCNN/mrcnn/model_conv2d.py\u001b[0m in \u001b[0;36mresnet_graph\u001b[0;34m(input_image, architecture, stage5, train_bn)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Stage 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bn_conv1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_bn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    879\u001b[0m               \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             inputs, outputs = self._set_connectivity_metadata_(\n\u001b[0;32m--> 881\u001b[0;31m                 inputs, outputs, args, kwargs)\n\u001b[0m\u001b[1;32m    882\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_set_connectivity_metadata_\u001b[0;34m(self, inputs, outputs, args, kwargs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m     \u001b[0;31m# This updates the layer history of the output tensor(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m     self._add_inbound_node(\n\u001b[0;32m-> 2043\u001b[0;31m         input_tensors=inputs, output_tensors=outputs, arguments=arguments)\n\u001b[0m\u001b[1;32m   2044\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_add_inbound_node\u001b[0;34m(self, input_tensors, output_tensors, arguments)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \"\"\"\n\u001b[1;32m   2058\u001b[0m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0;32m-> 2059\u001b[0;31m                                         input_tensors)\n\u001b[0m\u001b[1;32m   2060\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n\u001b[1;32m   2061\u001b[0m                                       input_tensors)\n",
      "\u001b[0;32m~/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 536\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 536\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/matterport-tf115/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   2056\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \"\"\"\n\u001b[0;32m-> 2058\u001b[0;31m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0m\u001b[1;32m   2059\u001b[0m                                         input_tensors)\n\u001b[1;32m   2060\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 5717\n",
    "#end_step = np.ceil(num_images / FLAGS.batch_size).astype(np.int32) * FLAGS.epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                             final_sparsity=0.80,\n",
    "                                                             begin_step=0,\n",
    "                                                             end_step=1000)\n",
    "}\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model.keras_model, **pruning_params)\n",
    "model_for_pruning.compile(optimizer=optimizer, loss=loss)\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def apply_pruning_to_conv2d(layer):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer,\n",
    "                                                        pruning_schedule=pruning_sched.ConstantSparsity(0.5, 0))\n",
    "    return layer\n",
    "\n",
    "model_for_pruning = tf.keras.models.clone_model(\n",
    "    model.keras_model,\n",
    "    clone_function=apply_pruning_to_conv2d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
